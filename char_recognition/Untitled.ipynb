{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apurvnit/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/apurvnit/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score, make_scorer, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from six.moves import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Data classification  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(folder):\n",
    "    valid_images = [\".jpg\", \".gif\", \".png\", \".tga\"]\n",
    "    all_char = os.listdir(folder)\n",
    "    imgs_data = []\n",
    "    Y = []\n",
    "    \n",
    "    for current_char in all_char:\n",
    "        char_dir = os.path.join(folder,current_char)\n",
    "        \n",
    "        for image_name in os.listdir(char_dir):\n",
    "            image_dir = os.path.join(char_dir, image_name)\n",
    "            ext = os.path.splitext(image_dir)[1]\n",
    "            \n",
    "            if ext.lower() not in valid_images:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                img = Image.open(image_dir)\n",
    "                img = np.array(img.resize((28,28),Image.ANTIALIAS))\n",
    "                img = img.reshape(28*28)/255\n",
    "                imgs_data.append(img)\n",
    "                Y.append(current_char)\n",
    "                \n",
    "            except:\n",
    "                print(\"unable to fetch image\")\n",
    "            \n",
    "    print(np.array(imgs_data).shape, np.array(Y).reshape(len(Y),1).shape)\n",
    "    return np.array(imgs_data,dtype=np.float32).T, np.array(Y).reshape(1,len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to fetch image\n",
      "unable to fetch image\n",
      "unable to fetch image\n",
      "unable to fetch image\n",
      "unable to fetch image\n",
      "(529114, 784) (529114, 1)\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_image(\"/home/apurvnit/datasets/not_mnist/notMNIST_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 0.418343\n",
      "standard deviation =  0.454233\n"
     ]
    }
   ],
   "source": [
    "print(\"mean =\",np.mean(X))\n",
    "print(\"standard deviation = \",np.std(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization - \n",
    "Making dataset of approx. zero mean and 1 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(X_var, Y_var):\n",
    "    X, Y = shuffle(X_var.T, Y_var.T, random_state=random.randint(0,9))\n",
    "    print(X.T.shape,Y.T.shape)\n",
    "    return X.T, Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 529114) (1, 529114)\n",
      "[[ 0.14509805  0.17647059  0.32549021 ...,  1.          0.          0.        ]\n",
      " [ 0.57254905  0.73333335  0.4509804  ...,  1.          0.          0.        ]\n",
      " [ 0.65882355  0.80392158  0.7019608  ...,  1.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.92941177 ...,  1.          0.          0.        ]\n",
      " [ 0.          0.          0.81176472 ...,  1.          0.          0.        ]\n",
      " [ 0.          0.          0.75294119 ...,  1.          0.          0.        ]] [['D' 'A' 'I' ..., 'A' 'F' 'G']]\n"
     ]
    }
   ],
   "source": [
    "X_1, Y_1 = randomize(X, Y)\n",
    "print(X_1, Y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(X, Y):\n",
    "    \n",
    "    Y_1 = LabelEncoder().fit_transform(Y.T)\n",
    "    Y_1 = Y_1.reshape(1, Y.shape[1])\n",
    "    onehot = OneHotEncoder(categorical_features = [1])\n",
    "    Y = onehot.fit_transform(Y_1) \n",
    "    X_1, Y_1 = randomize(X,Y)\n",
    "    X_train = X_1[:, 0:int(0.75*X_1.shape[1])]\n",
    "    Y_train = Y_1[:, 0:int(0.75 * Y_1.shape[1])]\n",
    "    X_test = X_1[:, int(0.75 * X_1.shape[1]):]\n",
    "    Y_test = Y_1[:, int(0.75 * X_1.shape[1]):]\n",
    "    \n",
    "    \n",
    "    return X_train,Y_train,X_test,Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apurvnit/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/label.py:129: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 529114) (1, 529114)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,X_test,Y_test = data_preprocessing(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we have made and classified data for our program. lets save it so that we dont have to do such a long work again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_dataset(pickle_file):\n",
    "\n",
    "    try:\n",
    "      f = open(pickle_file, 'wb')\n",
    "      save = {\n",
    "        'train_dataset': X_train,\n",
    "        'train_labels': Y_train,\n",
    "        'test_dataset': X_test,\n",
    "        'test_labels': Y_test,\n",
    "        }\n",
    "      pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "      f.close()\n",
    "    except Exception as e:\n",
    "      print('Unable to save data to', pickle_file, ':', e)\n",
    "      raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 1667133176\n"
     ]
    }
   ],
   "source": [
    "pickle_file = os.path.join(\"/home/apurvnit/Projects/My_deep-learning_examples/char_recognition\", 'char_recognition.pickle')\n",
    "save_dataset(pickle_file)\n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Second part of the model\n",
    "\n",
    "Since we have already preprocessed the data it would be better to directly call the variables from pickle file created before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = \"../../pickle_files/char_recognition.pickle\"\n",
    "\n",
    "with open(pickle_file,'rb') as file:\n",
    "    s = pickle.load(file)\n",
    "    X_train_1 = s['train_dataset']\n",
    "    Y_train_1 = s['train_labels']\n",
    "    X_test_1 = s['test_dataset']\n",
    "    Y_test_1 = s['test_labels']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restructuring variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 132279) (784, 132279)\n",
      "(396835, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_1.shape, X_test_1.shape)\n",
    "Y = Y_train_1.toarray()\n",
    "Y_train_2 = Y.reshape(Y.shape[-1])\n",
    "Y = Y_test_1.toarray()\n",
    "Y_test_2 = Y.reshape(Y.shape[-1])\n",
    "Y_train = (np.arange(10) == Y_train_2[:,None]).astype(np.float32)\n",
    "Y_test_3 = (np.arange(10) == Y_test_2[:,None]).astype(np.float32)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66139\n",
      "(132279, 10)\n",
      "(66140, 784) (66139, 784)\n",
      "(784, 396835)\n"
     ]
    }
   ],
   "source": [
    "valid_size = int(5 * Y_test_3.shape[0]/10)\n",
    "print(valid_size)\n",
    "Y_test = Y_test_3[:valid_size]\n",
    "print(Y_test_3.shape)\n",
    "Y_valid = Y_test_3[valid_size:]\n",
    "X_test = X_test_1.T[:valid_size]\n",
    "X_valid = X_test_1.T[valid_size:]\n",
    "print(X_valid.shape, X_test.shape)\n",
    "print(X_train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing tenserflow modules\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start writing code in tenserflow , you need to assign variables and constants and kind of attach it to tenserflow.\n",
    "Then, You can create a session and run these operations as many times as you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple gradient descent\n",
    "subset = 20000\n",
    "num_labels = 10\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_X = tf.constant(X_train_1.T[:subset])\n",
    "    tf_Y = tf.constant(Y_train[:subset])\n",
    "    tf_test_X = tf.constant(X_test)\n",
    "    tf_val_X = tf.constant(X_valid)\n",
    "\n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    tf_w = tf.Variable(tf.truncated_normal([28*28,num_labels]))\n",
    "    tf_b = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Computing training values with softmax cross entropy and logits\n",
    "    logits = tf.matmul(tf_X, tf_w) + tf_b\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=logits))\n",
    "\n",
    "    # now using gradient descent finding minimal loss\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_val_X, tf_w) + tf_b)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_X, tf_w) + tf_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 28.474848\n",
      "Training accuracy: 6.3%\n",
      "Validation accuracy: 6.1%\n",
      "Loss at step 100: 2.436307\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 71.2%\n",
      "Loss at step 200: 1.987762\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 73.7%\n",
      "Loss at step 300: 1.748384\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 400: 1.587049\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 500: 1.467205\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 75.2%\n",
      "Loss at step 600: 1.372812\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 75.4%\n",
      "Loss at step 700: 1.295781\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 75.6%\n",
      "Test accuracy: 75.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 800\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # simple stuff that happens in a forward and back prop\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(num_steps):\n",
    "        # running the optimizer using .run()\n",
    "        _, l, predictions = sess.run([optimizer, loss, train_prediction])\n",
    "        \n",
    "        if (i % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (i, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "            predictions, Y_train[:subset, :]))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), Y_valid))\n",
    "            \n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now using stochastic gradient descent with batch size as 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  tf_X = tf.placeholder(tf.float32, shape=(batch_size, 28*28))\n",
    "  tf_Y = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_val_X = tf.constant(X_valid)\n",
    "  tf_test_X = tf.constant(X_test)\n",
    "  \n",
    "  # Variables.\n",
    "  tf_w = tf.Variable(\n",
    "    tf.truncated_normal([28 * 28, num_labels]))\n",
    "  tf_b = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_X, tf_w) + tf_b\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_Y, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_val_X, tf_w) + tf_b)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_X, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 19.429384\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.3%\n",
      "Minibatch loss at step 500: 1.501353\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1000: 1.020827\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 1500: 1.996914\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 2000: 2.002791\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 2500: 1.184640\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 3000: 1.333805\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 75.0%\n",
      "Test accuracy: 74.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    " \n",
    "    offset = (step * batch_size) % (Y_train.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = X_train_1.T[offset:(offset + batch_size)]\n",
    "    batch_labels = Y_train[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_X : batch_data, tf_Y : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), Y_valid))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting the improvement in predicting our SGD model's valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib.plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-95878e384efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'matplotlib.plot'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_valid(valid_Y, range_Y):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
